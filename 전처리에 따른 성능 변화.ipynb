{"cells":[{"cell_type":"markdown","metadata":{"id":"o59tfZlU_TK5"},"source":["#기본 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15453,"status":"ok","timestamp":1646354214255,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"VKaxDKc2P7jr","outputId":"0a69c418-6d57-4691-8a9c-b47be846431a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1wWk1ctP_Ts"},"outputs":[],"source":["data_path = \"/content/drive/MyDrive/Colab Notebooks/ssac/deep/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOQ4CSdfQA1X"},"outputs":[],"source":["from tensorflow import keras\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import cv2\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import sklearn\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import torch\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCW7kPckQHPX"},"outputs":[],"source":["seed = 42\n","random.seed(seed)\n","np.random.seed(seed)\n","os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","tf.random.set_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m9tLbNn1QIZR"},"outputs":[],"source":["# 이미지의 가로 세로\n","IMG_SIZE = 224\n","# 비디오에서 학습할 프레임 개수\n","MAX_SEQ_LENGTH = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbe60SZPQJMm"},"outputs":[],"source":["train_df = pd.read_csv(data_path + 'train.csv')\n","train_df[\"label\"] = train_df[\"tag\"]\n","\n","for index, data in enumerate(train_df[\"label\"].unique()):\n","    # label 컬럼에 저장된 data 를 index로 변환\n","    train_df[\"label\"].replace(data, index, inplace=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnhKaMphQKKy"},"outputs":[],"source":["train_df = sklearn.utils.shuffle(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6fSOd_jQLKh"},"outputs":[],"source":["# 비디오를 구성하는 이미지의 가운데 부분을 리턴\n","# frame : 비디오를 구성하는 이미지\n","def crop_center_square(frame):\n","    # frame.shape[0:2] : 이미지의 세로, 가로 리턴\n","    y, x = frame.shape[0:2]\n","    # 이미지의 세로 가로 중에서 작은 값을 리턴\n","    min_dim = min(y, x)\n","    #이미지의 왼쪽 모서리 좌표의 가로 시작점\n","    # 이미지 가로 좌표 (이미지 가로 //2  - min_dim//2) ~ (이미지 가로 //2  + min_dim//2) \n","    start_x = (x // 2) - (min_dim // 2)\n","    # 이미지의 왼쪽 모서리 세로 좌표 시작점\n","    # 이미지 세로 좌표 (이미지 세로 //2  - min_dim//2) ~ (이미지 세로 //2  + min_dim//2) \n","    start_y = (y // 2) - (min_dim // 2)\n","    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C89hUj3oArlO"},"outputs":[],"source":["# 비디오 파일을 읽어서 각 프레임을 이미지로 변환해서 리턴\n","# path : 비디오 파일 경로\n","# max_frames : 이미지로 변환할 프레임수\n","# resize=(IMG_SIZE, IMG_SIZE) : 이미지의 가로 세로\n","def load_video(path, max_frames=20, resize=(IMG_SIZE, IMG_SIZE)):\n","    # 비디오 파일을 읽어서 이미지로 변환 할 객페\n","    cap = cv2.VideoCapture(path)\n","    frames = []\n","    try:\n","        while True:\n","            # cap.read() : 비디오를 읽어서 리턴\n","            # ret : 비디오 읽기가 성공했으면 True, 더이상 읽을 비디오 프레임이 없으면 False 가 리턴\n","            # frame : 비디오 프레임 이미지를 리턴\n","            ret, frame = cap.read()\n","            # ret 가 False면 반복 종료\n","            if not ret:\n","                break\n","            # 비디오 이미지를 가운데 리턴\n","            frame = crop_center_square(frame)\n","            #비디오 이미지의 가로 세로를 resize=(224,224) 로 변환\n","            frame = cv2.resize(frame, resize)\n","            # frame 은  [줄, 칸, B G R ] 로 구성되 있음 \n","            # frame [ : (모든줄), : (모든칸), R (인덱스2) G (인덱스1) B (인덱스0 )] 리턴\n","            frame = frame[:, :, [2, 1, 0]]\n","            # frame을 frames에 추가\n","            frames.append(frame)\n","            # frames에 저장된 데이터수가 max_frames와 같으면 종료\n","            if len(frames) == max_frames:\n","                break\n","    finally:\n","        # 비디오 이미지 변환 종료\n","        cap.release()\n","    # frames를 numpy 배열로 변환 해서 리턴\n","    return np.array(frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iAJ9KBRHHfj"},"outputs":[],"source":["# 모든 비디오 파일의 이미지와 종류를 리턴\n","# video_name : 비디오 파일명\n","# label: 비디오 파일 종류\n","# root_dir : 비디오 파일 경로\n","def prepare_all_videos(video_name , label, root_dir):\n","    # 비디오 파일 이름의 개수\n","    num_samples = len(video_name)\n","    # video_name.values.tolist() : 비디오 파일 이름을 리스트로 변환 해서 리턴\n","    video_paths = video_name.values.tolist()\n","    # 비디오 종류를 리턴\n","    labels = label.values\n","    # 비디오 종류를 2차원 배열로 변환\n","    labels = labels.reshape(-1,1)\n","    # 0으로 초기화된 [비디오 파일개수 * 20 , 224 , 224, 3] 배열 생성\n","    x = np.zeros(shape=(num_samples*MAX_SEQ_LENGTH, IMG_SIZE, IMG_SIZE, 3 ), dtype=\"float32\" )\n","    # 0으로 초기화된 [비디오파일개수 * 20] 인 배열 생성\n","    y = np.zeros(shape=(num_samples*MAX_SEQ_LENGTH), dtype=\"float32\")\n","    # 인덱스 초기화\n","    index = 0\n","    # 비디오 파일의 개수 만큼 반복\n","    for idx, path in enumerate(video_paths):\n","        # 비디오 파일을 읽어서 각 프레임의 이미지를 리턴\n","        frames = load_video(root_dir+ path)\n","        # 프레임의 개수 만큼 반복\n","        for i in range(len(frames)):\n","            # frame의 i번째 이미지를 x에 추가\n","            x[index] = np.array(frames[i], dtype=\"float32\")\n","            # 비디오의 종류가 저장된 labels의 idx 번째를 y에 추가\n","            y[index]= np.array(labels[idx], dtype=\"float32\")\n","            # index 1 증가\n","            index += 1 \n","    \n","    # 전체 비디오 프레임을 이미지로 변환한 x \n","    # to_categorical(y) : 이미지의 종류 y 를 onehot 인코딩 \n","    return (x,to_categorical(y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUV6CHsHHHh5"},"outputs":[],"source":["X_train, y_train = prepare_all_videos(train_df[\"video_name\"], train_df[\"label\"] , data_path + \"train/\")"]},{"cell_type":"markdown","metadata":{"id":"UqrZnkVS_cAa"},"source":["#증강하지 않은 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgz7_N4jQrS_"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3473,"status":"ok","timestamp":1646354444791,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"bWa4HVv4Q2MI","outputId":"8cfdad63-211b-4b1b-f08b-9eabf6004cb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 0s 0us/step\n","58900480/58889256 [==============================] - 0s 0us/step\n"]}],"source":["vggnet = VGG16( include_top=False,input_shape=(224, 224, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lUquemNRA6e"},"outputs":[],"source":["vggnet.trainable=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tp_ApKakRCU3"},"outputs":[],"source":["model = Sequential()\n","model.add(vggnet)\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.8))\n","model.add(Dense(3, activation=\"softmax\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1646319314557,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"Nig2AJhjRDXJ","outputId":"340fc9e6-ccbe-4923-f9bb-52f045b4c6a4"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["from tensorflow.keras.optimizers import Adam\n","model.compile(\n","                  loss=\"categorical_crossentropy\", \n","                   optimizer=Adam(lr=1e-4),\n","                   metrics=[\"acc\"]\n","              )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yz2YZxaIRErc"},"outputs":[],"source":["es = tf.keras.callbacks.EarlyStopping(monitor = 'loss', mode = 'min', verbose = 1, patience = 3)\n","save = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Colab Notebooks/ssac/deep/best_model_normal.h5\", monitor = 'loss', save_best_only=True)\n","callback = [es, save]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":240213,"status":"ok","timestamp":1646319805219,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"HWsBRtxrSsQi","outputId":"35b28316-9537-465a-8e43-08c9fb6c070c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","223/223 [==============================] - 27s 82ms/step - loss: 0.1399 - acc: 0.9552\n","Epoch 2/100\n","223/223 [==============================] - 19s 86ms/step - loss: 0.0099 - acc: 0.9990\n","Epoch 3/100\n","223/223 [==============================] - 20s 89ms/step - loss: 0.0051 - acc: 0.9997\n","Epoch 4/100\n","223/223 [==============================] - 20s 88ms/step - loss: 0.0031 - acc: 0.9999\n","Epoch 5/100\n","223/223 [==============================] - 19s 87ms/step - loss: 0.0025 - acc: 1.0000\n","Epoch 6/100\n","223/223 [==============================] - 19s 86ms/step - loss: 0.0017 - acc: 1.0000\n","Epoch 7/100\n","223/223 [==============================] - 20s 88ms/step - loss: 0.0013 - acc: 1.0000\n","Epoch 8/100\n","223/223 [==============================] - 19s 85ms/step - loss: 9.6502e-04 - acc: 1.0000\n","Epoch 9/100\n","223/223 [==============================] - 19s 85ms/step - loss: 7.9224e-04 - acc: 1.0000\n","Epoch 10/100\n","223/223 [==============================] - 18s 79ms/step - loss: 0.0010 - acc: 1.0000\n","Epoch 11/100\n","223/223 [==============================] - 18s 79ms/step - loss: 0.0015 - acc: 1.0000\n","Epoch 12/100\n","223/223 [==============================] - 17s 78ms/step - loss: 0.0046 - acc: 0.9990\n","Epoch 12: early stopping\n"]}],"source":["hist = model.fit(\n","\t#train_sequence가 리턴하는 증강된 이미지를 학습  \n","\tX_train/255, y_train, batch_size = 32,\n","    # 5번 반복해서 전체 이미지 학습 \n","\tepochs = 100, callbacks = callback)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FRGvf55Svau"},"outputs":[],"source":["model = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/ssac/deep/best_model_normal.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuG4uuH5T61-"},"outputs":[],"source":["from keras.callbacks import EarlyStopping\n","# 테스트 비디오 파일명과 종류 리턴\n","test_df = pd.read_csv(data_path+\"test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYDeJtoyT-XK"},"outputs":[],"source":["test_df[\"label\"] = test_df[\"tag\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzK8-GKZT_Qi"},"outputs":[],"source":["text_label = test_df[\"label\"].unique()\n","# enumerate(test_df[\"label\"].unique()) : label 컬럼에 저장된 데이터를 중복을 제거한 값에 인덱스 추가\n","for index, data in enumerate(test_df[\"label\"].unique()):\n","    # label 컬럼에 저장된 data 를 index로 변환\n","    test_df[\"label\"].replace(data, index, inplace=True) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6dC9df1gUAQI"},"outputs":[],"source":["test_df=sklearn.utils.shuffle(test_df)\n","# sklearn 배열 혼합"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSSreJhFUCU2"},"outputs":[],"source":["X_test, y_test = prepare_all_videos(test_df[\"video_name\"], test_df[\"label\"] , data_path + \"test/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8213,"status":"ok","timestamp":1646320030721,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"8SEmjVOPUDjw","outputId":"f35ae2c9-67e6-432c-ab77-46073f44eaed"},"outputs":[{"name":"stdout","output_type":"stream","text":["86/86 [==============================] - 7s 84ms/step - loss: 0.3487 - acc: 0.8788\n"]},{"data":{"text/plain":["[0.3487128019332886, 0.8788321018218994]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model.evaluate(\n","    X_test/255, y_test, batch_size = 32\n",")\n","# .evaluate 함수를 통해 모델 정답률, loss값 도출"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UlTFlzSVUHvA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"2edbSiPt_hap"},"source":["accuracy 0.8788, loss 0.3487"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWpPPikn_ma3"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"38L3m3F0_m4i"},"source":["# 데이터 증강"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3090,"status":"ok","timestamp":1646355945992,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"0J10lJ1-6Rv4","outputId":"b8894d67-6743-4d24-edd7-0fff676b70a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.18.3)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (1.8.1.post1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n"]}],"source":["!pip install six numpy scipy Pillow matplotlib scikit-image opencv-python imageio Shapely"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2631,"status":"ok","timestamp":1646355948615,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"mjOE4uM-6XWB","outputId":"9615d70e-50b3-4ed5-9e9c-95187e52171f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: imgaug in /usr/local/lib/python3.7/dist-packages (0.2.9)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug) (0.18.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.15.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug) (4.1.2.30)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.8.1.post1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug) (3.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.4.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug) (2.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from imgaug) (1.21.5)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug) (2021.11.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug) (2.8.2)\n"]}],"source":["!pip install imgaug"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EEd6zRYuQRKT"},"outputs":[],"source":["import imgaug.augmenters as iaa\n","import imgaug as ia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLLcHRTSQXZE"},"outputs":[],"source":["# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n","# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n","sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","# Define our sequence of augmentation steps that will be applied to every image\n","# All augmenters with per_channel=0.5 will sample one value _per image_\n","# in 50% of all cases. In all other cases they will sample new values\n","# _per channel_.\n","\n","seq = iaa.Sequential(\n","    [\n","        # apply the following augmenters to most images\n","        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n","        iaa.Flipud(0.2), # vertically flip 20% of all images\n","        # crop images by -5% to 10% of their height/width\n","        sometimes(iaa.CropAndPad(\n","            percent=(-0.05, 0.1),\n","            pad_mode=ia.ALL,\n","            pad_cval=(0, 255)\n","        )),\n","        sometimes(iaa.Affine(\n","            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n","            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n","            rotate=(-45, 45), # rotate by -45 to +45 degrees\n","            shear=(-16, 16), # shear by -16 to +16 degrees\n","            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n","            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n","            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n","        )),\n","        # execute 0 to 5 of the following (less important) augmenters per image\n","        # don't execute all of them, as that would often be way too strong\n","        iaa.SomeOf((0, 5),\n","            [\n","                sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n","                iaa.OneOf([\n","                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n","                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n","                    iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n","                ]),\n","                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n","                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n","                # search either for all edges or for directed edges,\n","                # blend the result with the original image using a blobby mask\n","                iaa.SimplexNoiseAlpha(iaa.OneOf([\n","                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n","                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n","                ])),\n","                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n","                iaa.OneOf([\n","                    iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n","                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n","                ]),\n","                iaa.Invert(0.05, per_channel=True), # invert color channels\n","                iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n","                iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n","                # either change the brightness of the whole image (sometimes\n","                # per channel) or change the brightness of subareas\n","                iaa.OneOf([\n","                    iaa.Multiply((0.5, 1.5), per_channel=0.5),\n","                    iaa.FrequencyNoiseAlpha(\n","                        exponent=(-4, 0),\n","                        first=iaa.Multiply((0.5, 1.5), per_channel=True),\n","                        second=iaa.LinearContrast((0.5, 2.0))\n","                    )\n","                ]),\n","                iaa.LinearContrast((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n","                iaa.Grayscale(alpha=(0.0, 1.0)),\n","                sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n","                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n","                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n","            ],\n","            random_order=True\n","        )\n","    ],\n","    random_order=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLSNkJGhQZsW"},"outputs":[],"source":["import math"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQHEwoviPBFm"},"outputs":[],"source":["# 이미지를 증강시켜서 배치 사이즈 만큼씩 리턴하는 클래스 구현\n","\n","class  ImageSequence(tf.keras.utils.Sequence):\n","    # 객체 생성시 실행되는 함수\n","    # 매개변수\n","    # image_arr : 이미지가 저장된 배열\n","    # label_arr : 이미지의 종류가 저장된 배열\n","    # batch_size : 배치 사이즈 (한번에 리턴할 파일 개수)\n","    # seq : 이미지 증강 객체\n","\n","    def __init__(self, image_arr ,label_arr, batch_size, seq):\n","        # 매개변수들을 속성에 저장\n","        self.image_arr  = image_arr \n","        self.label_arr = label_arr\n","        self.batch_size = batch_size\n","        self.seq = seq\n","    \n","    # batch_size씩 이미지를 리턴했을때 전체 이미지를 리턴하려명 몆번 반복해야 하는지 리턴\n","    def __len__(self):\n","        \n","        # math.ceil : 소숫점 1자리 올림 예) 6.1 -> 7   6 -> 6   6.0 -> 6\n","\n","        # image_arr  (이미지가 저장된 배열) / batch_size (한번에 리턴할 이미지 개수) 의 올림을 리턴     \n","        return math.ceil(len(self.image_arr ) / self.batch_size)\n","\n","    # 학습시 batch_size 씩 이미지를 리턴하는 함수로 텐서플로우에서 학습시 model 객체에서 자동으로 호출하는 함수\n","    # 매개변수 idx : 몆번째 batch 인지가 저장되는 매개변수 0부터 시작\n","    def __getitem__(self, idx):\n","        # idx * self.batch_size 부터 (idx + 1) * self.batch_size 까지 이미지 리턴\n","        batch_image_arr = self.image_arr[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        # 증강시키기 위해서 batch_image_arr을 정수로 변환\n","        img = np.array( batch_image_arr,  dtype=np.uint8)\n","        # img를 증강시켜서 generate_img에 저장\n","        generate_img = seq(images=img) \n","        # idx * self.batch_size 부터 (idx + 1) * self.batch_size 까지 이미지 종류 리턴\n","        batch_label_arr = self.label_arr[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        return generate_img/255.0, batch_label_arr\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4VkrkFMAMG7"},"outputs":[],"source":["train_sequence = ImageSequence(X_train, y_train, 32, seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdiu2m1_UmQr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW2nclPHUsWg"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1GMwc5IUsWh"},"outputs":[],"source":["vggnet = VGG16( include_top=False,input_shape=(224, 224, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uuafr5FUsWh"},"outputs":[],"source":["vggnet.trainable=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ToTXW33UsWh"},"outputs":[],"source":["model = Sequential()\n","model.add(vggnet)\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.8))\n","model.add(Dense(3, activation=\"softmax\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1646320096136,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"RZLliRwKUsWi","outputId":"b436ef30-7319-44e2-d98e-111f5e93efa1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["from tensorflow.keras.optimizers import Adam\n","model.compile(\n","                  loss=\"categorical_crossentropy\", \n","                   optimizer=Adam(lr=1e-4),\n","                   metrics=[\"acc\"]\n","              )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShEB-cpwUsWi"},"outputs":[],"source":["es = tf.keras.callbacks.EarlyStopping(monitor = 'loss', mode = 'min', verbose = 1, patience = 3)\n","save = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Colab Notebooks/ssac/deep/best_model_aug.h5\", monitor = 'loss', save_best_only=True)\n","callback = [es, save]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2283615,"status":"ok","timestamp":1646322406472,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"3HYYhdCnUsWj","outputId":"1bac912e-4f4d-4ec6-aa2c-fbe5df902ca8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/segmentation.py:191: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n","  segments = segmentation.slic(image, n_segments=n_segments_samples[i], compactness=10)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","223/223 [==============================] - 111s 496ms/step - loss: 0.9201 - acc: 0.6233\n","Epoch 2/100\n","223/223 [==============================] - 118s 529ms/step - loss: 0.5343 - acc: 0.7868\n","Epoch 3/100\n","223/223 [==============================] - 115s 514ms/step - loss: 0.4223 - acc: 0.8360\n","Epoch 4/100\n","223/223 [==============================] - 110s 495ms/step - loss: 0.3788 - acc: 0.8569\n","Epoch 5/100\n","223/223 [==============================] - 113s 508ms/step - loss: 0.3272 - acc: 0.8771\n","Epoch 6/100\n","223/223 [==============================] - 115s 516ms/step - loss: 0.3191 - acc: 0.8782\n","Epoch 7/100\n","223/223 [==============================] - 116s 522ms/step - loss: 0.2861 - acc: 0.8909\n","Epoch 8/100\n","223/223 [==============================] - 113s 504ms/step - loss: 0.2866 - acc: 0.8969\n","Epoch 9/100\n","223/223 [==============================] - 112s 504ms/step - loss: 0.2612 - acc: 0.9114\n","Epoch 10/100\n","223/223 [==============================] - 112s 502ms/step - loss: 0.2844 - acc: 0.8966\n","Epoch 11/100\n","223/223 [==============================] - 112s 501ms/step - loss: 0.2599 - acc: 0.9072\n","Epoch 12/100\n","223/223 [==============================] - 112s 501ms/step - loss: 0.2470 - acc: 0.9131\n","Epoch 13/100\n","223/223 [==============================] - 112s 504ms/step - loss: 0.2325 - acc: 0.9154\n","Epoch 14/100\n","223/223 [==============================] - 112s 501ms/step - loss: 0.2271 - acc: 0.9162\n","Epoch 15/100\n","223/223 [==============================] - 116s 519ms/step - loss: 0.2344 - acc: 0.9131\n","Epoch 16/100\n","223/223 [==============================] - 112s 503ms/step - loss: 0.2243 - acc: 0.9154\n","Epoch 17/100\n","223/223 [==============================] - 110s 491ms/step - loss: 0.2008 - acc: 0.9291\n","Epoch 18/100\n","223/223 [==============================] - 114s 511ms/step - loss: 0.2090 - acc: 0.9218\n","Epoch 19/100\n","223/223 [==============================] - 111s 495ms/step - loss: 0.2038 - acc: 0.9247\n","Epoch 20/100\n","223/223 [==============================] - 109s 490ms/step - loss: 0.2009 - acc: 0.9243\n","Epoch 20: early stopping\n"]}],"source":["hist = model.fit(\n","\t#train_sequence가 리턴하는 증강된 이미지를 학습  \n","\ttrain_sequence,\n","    # 5번 반복해서 전체 이미지 학습 \n","\tepochs = 100, callbacks = callback)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FntV5UVnUsWj"},"outputs":[],"source":["model = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/ssac/deep/best_model_aug.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11696,"status":"ok","timestamp":1646322423722,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"6rgnIRFjUsWj","outputId":"ae23bb4b-d7ec-4343-da63-a8698e5a6090"},"outputs":[{"name":"stdout","output_type":"stream","text":["86/86 [==============================] - 7s 78ms/step - loss: 0.2271 - acc: 0.9186\n"]},{"data":{"text/plain":["[0.22714002430438995, 0.9186131358146667]"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["model.evaluate(\n","    X_test/255, y_test, batch_size = 32\n",")"]},{"cell_type":"markdown","metadata":{"id":"3fYSCMAV_txB"},"source":["accuracy 0.9186, loss 0.2271"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K0aLBNOqU7Bp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NsNzLkaP_zFH"},"source":["#데이터 증강 + cutmix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTwAc-vaVhN6"},"outputs":[],"source":["import imgaug.augmenters as iaa\n","import imgaug as ia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrMgO0lQVhN7"},"outputs":[],"source":["# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n","# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second image.\n","sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n","\n","# Define our sequence of augmentation steps that will be applied to every image\n","# All augmenters with per_channel=0.5 will sample one value _per image_\n","# in 50% of all cases. In all other cases they will sample new values\n","# _per channel_.\n","\n","seq = iaa.Sequential(\n","    [\n","        # apply the following augmenters to most images\n","        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n","        iaa.Flipud(0.2), # vertically flip 20% of all images\n","        # crop images by -5% to 10% of their height/width\n","        sometimes(iaa.CropAndPad(\n","            percent=(-0.05, 0.1),\n","            pad_mode=ia.ALL,\n","            pad_cval=(0, 255)\n","        )),\n","        sometimes(iaa.Affine(\n","            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}, # scale images to 80-120% of their size, individually per axis\n","            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}, # translate by -20 to +20 percent (per axis)\n","            rotate=(-45, 45), # rotate by -45 to +45 degrees\n","            shear=(-16, 16), # shear by -16 to +16 degrees\n","            order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n","            cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n","            mode=ia.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n","        )),\n","        # execute 0 to 5 of the following (less important) augmenters per image\n","        # don't execute all of them, as that would often be way too strong\n","        iaa.SomeOf((0, 5),\n","            [\n","                sometimes(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n","                iaa.OneOf([\n","                    iaa.GaussianBlur((0, 3.0)), # blur images with a sigma between 0 and 3.0\n","                    iaa.AverageBlur(k=(2, 7)), # blur image using local means with kernel sizes between 2 and 7\n","                    iaa.MedianBlur(k=(3, 11)), # blur image using local medians with kernel sizes between 2 and 7\n","                ]),\n","                iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n","                iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n","                # search either for all edges or for directed edges,\n","                # blend the result with the original image using a blobby mask\n","                iaa.SimplexNoiseAlpha(iaa.OneOf([\n","                    iaa.EdgeDetect(alpha=(0.5, 1.0)),\n","                    iaa.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n","                ])),\n","                iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5), # add gaussian noise to images\n","                iaa.OneOf([\n","                    iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n","                    iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n","                ]),\n","                iaa.Invert(0.05, per_channel=True), # invert color channels\n","                iaa.Add((-10, 10), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n","                iaa.AddToHueAndSaturation((-20, 20)), # change hue and saturation\n","                # either change the brightness of the whole image (sometimes\n","                # per channel) or change the brightness of subareas\n","                iaa.OneOf([\n","                    iaa.Multiply((0.5, 1.5), per_channel=0.5),\n","                    iaa.FrequencyNoiseAlpha(\n","                        exponent=(-4, 0),\n","                        first=iaa.Multiply((0.5, 1.5), per_channel=True),\n","                        second=iaa.LinearContrast((0.5, 2.0))\n","                    )\n","                ]),\n","                iaa.LinearContrast((0.5, 2.0), per_channel=0.5), # improve or worsen the contrast\n","                iaa.Grayscale(alpha=(0.0, 1.0)),\n","                sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n","                sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n","                sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n","            ],\n","            random_order=True\n","        )\n","    ],\n","    random_order=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrqEdjiCVhN8"},"outputs":[],"source":["import math"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hvz3nJUHVhN8"},"outputs":[],"source":["# 이미지를 증강시켜서 배치 사이즈 만큼씩 리턴하는 클래스 구현\n","\n","class  ImageSequence(tf.keras.utils.Sequence):\n","    # 객체 생성시 실행되는 함수\n","    # 매개변수\n","    # image_arr : 이미지가 저장된 배열\n","    # label_arr : 이미지의 종류가 저장된 배열\n","    # batch_size : 배치 사이즈 (한번에 리턴할 파일 개수)\n","    # seq : 이미지 증강 객체\n","\n","    def __init__(self, image_arr ,label_arr, batch_size, seq):\n","        # 매개변수들을 속성에 저장\n","        self.image_arr  = image_arr \n","        self.label_arr = label_arr\n","        self.batch_size = batch_size\n","        self.seq = seq\n","    \n","    # batch_size씩 이미지를 리턴했을때 전체 이미지를 리턴하려명 몆번 반복해야 하는지 리턴\n","    def __len__(self):\n","        \n","        # math.ceil : 소숫점 1자리 올림 예) 6.1 -> 7   6 -> 6   6.0 -> 6\n","\n","        # image_arr  (이미지가 저장된 배열) / batch_size (한번에 리턴할 이미지 개수) 의 올림을 리턴     \n","        return math.ceil(len(self.image_arr ) / self.batch_size)\n","\n","    # 학습시 batch_size 씩 이미지를 리턴하는 함수로 텐서플로우에서 학습시 model 객체에서 자동으로 호출하는 함수\n","    # 매개변수 idx : 몆번째 batch 인지가 저장되는 매개변수 0부터 시작\n","    def __getitem__(self, idx):\n","        # idx * self.batch_size 부터 (idx + 1) * self.batch_size 까지 이미지 리턴\n","        images = self.image_arr[idx * self.batch_size:(idx + 1) * self.batch_size]\n","        # 증강시키기 위해서 batch_image_arr을 정수로 변환\n","        img = np.array( images,  dtype=np.uint8)\n","        # img를 증강시켜서 generate_img에 저장\n","        generate_img = seq(images=img) \n","        # idx * self.batch_size 부터 (idx + 1) * self.batch_size 까지 이미지 종류 리턴\n","        labels = self.label_arr[idx * self.batch_size:(idx + 1) * self.batch_size]\n","\n","        \n","        generate_img = generate_img/255\n","        #====================\n","       # print(\"자를꺼여\")\n","        #print(\"=\"*100)\n","        imgs = []; labs = []\n","        for i in range(len(generate_img)):   #i = 자를 사진 index\n","          #print(\"i = \", i)\n","          APPLY = tf.cast(tf.random.uniform(()) >= 0.5, tf.int32)  #0-1 랜덤 숫자 0.5보다 크면 1 아니면 0\n","          idx = tf.random.uniform((), 0, len(generate_img), tf.int32)  #채워줄 사진 index,  0-배치 중 하나\n","\n","          W = 224\n","          H = 224\n","          lam = tf.random.uniform(())  #사진 비율\n","          cut_ratio = tf.math.sqrt(1.-lam)\n","          cut_w = tf.cast(W * cut_ratio, tf.int32) * APPLY   #0곱해지면 적용 x\n","          cut_h = tf.cast(H * cut_ratio, tf.int32) * APPLY\n","\n","          cx = tf.random.uniform((), int(W/8), int(7/8*W), tf.int32)   #자르는 이미지의 중심좌표(중심이 너무 외곽이 안잡히도록)\n","          cy = tf.random.uniform((), int(H/8), int(7/8*H), tf.int32)\n","\n","          xmin = tf.clip_by_value(cx - cut_w//2, 0, W)  #0과 끝을 벗어나지 않도록\n","          ymin = tf.clip_by_value(cy - cut_h//2, 0, H)\n","          xmax = tf.clip_by_value(cx + cut_w//2, 0, W)\n","          ymax = tf.clip_by_value(cy + cut_h//2, 0, H)\n","\n","          #print(\"출력1\")\n","          mid_left = generate_img[i, ymin:ymax, :xmin, :]\n","         # print(\"idx = \", idx)\n","          mid_mid = generate_img[idx, ymin:ymax, xmin:xmax, :] \n","          #print(\"mid_mid = \", mid_mid)         \n","          mid_right = generate_img[i, ymin:ymax, xmax:, :]\n","          #print(\"mid_right = \", mid_right)   \n","\n","          middle = tf.concat([mid_left, mid_mid, mid_right], axis=1)\n","          top = generate_img[i, :ymin, :, :]\n","          bottom = generate_img[i, ymax:, :, :]\n","          new_img = tf.concat([top, middle, bottom], axis=0)\n","          imgs.append(new_img)\n","\n","          cut_w_mod = xmax - xmin   #자르는 이미지가 기존 이미지의 밖으로 나가는 경우 처리하기위해\n","          cut_h_mod = ymax - ymin\n","          alpha = tf.cast((cut_w_mod*cut_h_mod)/(W*H), tf.float32) #실제 잘려진 면적의 비율\n","          label1 = labels[i]\n","          label2 = labels[idx]\n","          new_label = ((1-alpha)*label1 + alpha*label2)\n","          labs.append(new_label)\n","         # print(\"new_label 추가!!!\")\n","\n","        #print(\"np.array(imgs) = \", np.array(imgs).shape )\n","        \n","        # new_imgs = tf.reshape(np.stack(imgs), [-1, 224, 224, 3])\n","        # new_labs = tf.reshape(np.stack(labs), [-1, 3])\n","\n","        return np.array(imgs, dtype=\"float32\"), np.array(labs,  dtype=\"float32\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"De2B9Ty-VhN8"},"outputs":[],"source":["train_sequence = ImageSequence(X_train, y_train, 32, seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ifv1HDKRVhN9"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aI1m7mMVhN9"},"outputs":[],"source":["vggnet = VGG16( include_top=False,input_shape=(224, 224, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ns58vAK2VhN9"},"outputs":[],"source":["vggnet.trainable=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CRgOUpQqVhN9"},"outputs":[],"source":["model = Sequential()\n","model.add(vggnet)\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dropout(0.8))\n","model.add(Dense(3, activation=\"softmax\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1646322424308,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"rJzLLDwtVhN-","outputId":"02185893-f555-44dc-a3f5-01392885a5e6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}],"source":["from tensorflow.keras.optimizers import Adam\n","model.compile(\n","                  loss=\"categorical_crossentropy\", \n","                   optimizer=Adam(lr=1e-4),\n","                   metrics=[\"acc\"]\n","              )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwBVtBvWVhN-"},"outputs":[],"source":["es = tf.keras.callbacks.EarlyStopping(monitor = 'loss', mode = 'min', verbose = 1, patience = 3)\n","save = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/MyDrive/Colab Notebooks/ssac/deep/best_model_cutmix.h5\", monitor = 'loss', save_best_only=True)\n","callback = [es, save]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4135561,"status":"ok","timestamp":1646326559864,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"Rs0nyeCVVhN-","outputId":"ff2b4737-1072-42d9-eba5-90ea7a67dc98"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/imgaug/augmenters/segmentation.py:191: FutureWarning: skimage.measure.label's indexing starts from 0. In future version it will start from 1. To disable this warning, explicitely set the `start_label` parameter to 1.\n","  segments = segmentation.slic(image, n_segments=n_segments_samples[i], compactness=10)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","223/223 [==============================] - 150s 671ms/step - loss: 1.1284 - acc: 0.5261\n","Epoch 2/100\n","223/223 [==============================] - 145s 650ms/step - loss: 0.6894 - acc: 0.6989\n","Epoch 3/100\n","223/223 [==============================] - 150s 669ms/step - loss: 0.5763 - acc: 0.7626\n","Epoch 4/100\n","223/223 [==============================] - 148s 664ms/step - loss: 0.5099 - acc: 0.7940\n","Epoch 5/100\n","223/223 [==============================] - 147s 660ms/step - loss: 0.4778 - acc: 0.8024\n","Epoch 6/100\n","223/223 [==============================] - 151s 679ms/step - loss: 0.4435 - acc: 0.8218\n","Epoch 7/100\n","223/223 [==============================] - 149s 670ms/step - loss: 0.4221 - acc: 0.8226\n","Epoch 8/100\n","223/223 [==============================] - 150s 670ms/step - loss: 0.4079 - acc: 0.8331\n","Epoch 9/100\n","223/223 [==============================] - 148s 661ms/step - loss: 0.3998 - acc: 0.8364\n","Epoch 10/100\n","223/223 [==============================] - 145s 648ms/step - loss: 0.3793 - acc: 0.8462\n","Epoch 11/100\n","223/223 [==============================] - 144s 644ms/step - loss: 0.3483 - acc: 0.8569\n","Epoch 12/100\n","223/223 [==============================] - 148s 664ms/step - loss: 0.3573 - acc: 0.8559\n","Epoch 13/100\n","223/223 [==============================] - 148s 665ms/step - loss: 0.3563 - acc: 0.8541\n","Epoch 14/100\n","223/223 [==============================] - 147s 657ms/step - loss: 0.3286 - acc: 0.8681\n","Epoch 15/100\n","223/223 [==============================] - 146s 655ms/step - loss: 0.3144 - acc: 0.8750\n","Epoch 16/100\n","223/223 [==============================] - 147s 658ms/step - loss: 0.3075 - acc: 0.8805\n","Epoch 17/100\n","223/223 [==============================] - 141s 630ms/step - loss: 0.3164 - acc: 0.8681\n","Epoch 18/100\n","223/223 [==============================] - 149s 666ms/step - loss: 0.2962 - acc: 0.8838\n","Epoch 19/100\n","223/223 [==============================] - 148s 661ms/step - loss: 0.3123 - acc: 0.8767\n","Epoch 20/100\n","223/223 [==============================] - 146s 656ms/step - loss: 0.2933 - acc: 0.8853\n","Epoch 21/100\n","223/223 [==============================] - 149s 668ms/step - loss: 0.3024 - acc: 0.8785\n","Epoch 22/100\n","223/223 [==============================] - 151s 674ms/step - loss: 0.2902 - acc: 0.8840\n","Epoch 23/100\n","223/223 [==============================] - 147s 661ms/step - loss: 0.2865 - acc: 0.8860\n","Epoch 24/100\n","223/223 [==============================] - 148s 662ms/step - loss: 0.3002 - acc: 0.8855\n","Epoch 25/100\n","223/223 [==============================] - 148s 665ms/step - loss: 0.2713 - acc: 0.8937\n","Epoch 26/100\n","223/223 [==============================] - 152s 681ms/step - loss: 0.2924 - acc: 0.8815\n","Epoch 27/100\n","223/223 [==============================] - 147s 659ms/step - loss: 0.2720 - acc: 0.8902\n","Epoch 28/100\n","223/223 [==============================] - 145s 652ms/step - loss: 0.2714 - acc: 0.8904\n","Epoch 28: early stopping\n"]}],"source":["hist = model.fit(\n","\t#train_sequence가 리턴하는 증강된 이미지를 학습  \n","\ttrain_sequence,\n","    # 5번 반복해서 전체 이미지 학습 \n","\tepochs = 100, callbacks = callback)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMNLlqebVhN_"},"outputs":[],"source":["model = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/ssac/deep/best_model_cutmix.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8792,"status":"ok","timestamp":1646355177718,"user":{"displayName":"hy o","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03196122990432239993"},"user_tz":-540},"id":"kNRApUI8YrnH","outputId":"72ab317c-ebd4-4e01-eb0e-a3e371e5f867"},"outputs":[{"name":"stdout","output_type":"stream","text":["86/86 [==============================] - 7s 84ms/step - loss: 0.2062 - acc: 0.9201\n"]},{"data":{"text/plain":["[0.2062472552061081, 0.9200729727745056]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":[" model.evaluate(\n","    X_test/255, y_test, batch_size = 32\n",")"]},{"cell_type":"markdown","metadata":{"id":"tjCWPmsqAAIv"},"source":["accuracy 0.9201  loss 0.2062"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljNwtE4fADv1"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNsFxhA5zgjcpYe+Mvswpyt","collapsed_sections":[],"machine_shape":"hm","name":"전처리에 따른 성능 변화.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
